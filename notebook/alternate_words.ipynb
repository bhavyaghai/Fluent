{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "from numpy.linalg import norm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import pearsonr\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(a, b):\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    return dot_product / (norm_a * norm_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_vector(vec):\n",
    "    return vec/norm(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model =  word2vec.KeyedVectors.load_word2vec_format('./data/word2vec_50k.bin', binary=True)\n",
    "model = KeyedVectors.load_word2vec_format('../data/word_embeddings/glove.wikipedia.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Calculating Bias</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tolga_bias(pairs, num_components = 10):\n",
    "    matrix = []\n",
    "    for a, b in pairs:\n",
    "        center = (model[a] + model[b])/2\n",
    "        matrix.append(model[a] - center)\n",
    "        matrix.append(model[b] - center)\n",
    "    matrix = np.array(matrix)\n",
    "    pca = PCA(n_components = num_components)\n",
    "    pca.fit(matrix)\n",
    "    return list(pca.components_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bias_score_by_word(w):\n",
    "    bias = None\n",
    "    if w in model:\n",
    "        bias = float(round(cos_sim(g,model[w]),5))\n",
    "    elif w.lower() in model:\n",
    "        t = w.lower()\n",
    "        bias = float(round(cos_sim(g,model[t]),5))\n",
    "    return bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1_words = ['girl','woman','she','mother','daughter','gal','female','her','herself']\n",
    "g2_words = ['boy','man','he','father','son','guy','male','his','himself']\n",
    "g = tolga_bias(zip(g1_words, g2_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.1875222, 0.26396933)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(g, list(model[\"engineer\"])), cos_sim(g, list(model[\"nurse\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.18752"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_bias_score_by_word(\"engineer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.16632304, 0.08470304)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(g, list(model[\"gangster\"])), cos_sim(g, list(model[\"teacher\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.050496504"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(g, list(model[\"riley\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Searching Alternates</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "# We have used two sources to get alternates: thesaurus and word embedding\n",
    "# To evaluate best alternates, we have two metrics: similarity sscore & bias score\n",
    "# we want words with high similarity & least bias (at least lesser than initial bias)\n",
    "# Based on empirical study of few sample words, We assume that synoyms produced by thesaurus are better than\n",
    "the ones prduced by word embeddings.\n",
    "# Hence synonyms from thesaurus are discarded only when its correspoding bias score is greater than initial bias\n",
    "or the word is not present in word embedding model\n",
    "Furthermore, Thesaurus doesn't provide any quantitative score for the synonyms so its difficult to compare similarity.\n",
    "\n",
    "# For synonyms extracted from word embedding, we compare bias score and similarity score\n",
    "# Ideally, we should find pareto optimal front to find word with highest similarity & least bias\n",
    "# In our case, we consider all synonyms whose similarity>threshold and whose bias score is less than initial bias\n",
    "# finally, we sort by bias and discard words with higher bias\n",
    "\n",
    "'''\n",
    "from thesaurus import Word\n",
    "import operator\n",
    "def alternates(name, max_results = 10):\n",
    "    init_bias = abs(get_bias_score_by_word(name))\n",
    "    neigh_thesau = {}\n",
    "    neigh = None\n",
    "    try:\n",
    "        w = Word(name)\n",
    "        neigh = w.synonyms()\n",
    "    except:\n",
    "        print(\"Not Found in Thesaurus !!!\")\n",
    "    \n",
    "    # if synonyms for the word are available\n",
    "    if neigh:\n",
    "        for x in neigh:\n",
    "            if x not in model:\n",
    "                continue\n",
    "            word_bias = abs(get_bias_score_by_word(x))\n",
    "            if word_bias<init_bias:\n",
    "                neigh_thesau[x] = word_bias\n",
    "    # if we get sufficient results from thesaurus return them (skip synonyms from word embedding)\n",
    "    if len(neigh_thesau)>=max_results:\n",
    "        res = sorted(neigh_thesau.items(), key=lambda kv: kv[1])\n",
    "        return res[:max_results]\n",
    "    neigh_embd = {}\n",
    "    synonym_limit_embedding = 2*max_results\n",
    "    min_semantic_sim = 0.60\n",
    "    neigh = model.similar_by_word(name, topn=synonym_limit_embedding)\n",
    "    for w,sim in neigh:\n",
    "        word_bias = abs(get_bias_score_by_word(w))\n",
    "        # If word is not already counted by thesaurus &\n",
    "        # word semantic similarity is greater than some threshold &\n",
    "        # bias score of synonym is less than the specific word\n",
    "        if sim<min_semantic_sim:\n",
    "            break\n",
    "        if w not in neigh_thesau and word_bias<init_bias:\n",
    "            neigh_embd[w] = word_bias\n",
    "    \n",
    "    neigh_thesau = sorted(neigh_thesau.items(), key=lambda kv: kv[1])\n",
    "    print(len(neigh_thesau))\n",
    "    #more_needed = max_results-len(neigh_thesau)\n",
    "    neigh_embd = sorted(neigh_embd.items(), key=lambda kv: kv[1])\n",
    "    print(len(neigh_embd))\n",
    "    res = (neigh_thesau + neigh_embd)[:max_results]\n",
    "    #sorted_res = sorted(res.items(), key=lambda value: value[1])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('pundit', 0.008390000090003014),\n",
       " ('instructor', 0.010459999553859234),\n",
       " ('teach', 0.024460000917315483),\n",
       " ('scholar', 0.042819999158382416),\n",
       " ('lecturer', 0.044179998338222504),\n",
       " ('professor', 0.04448999837040901),\n",
       " ('guide', 0.04552000015974045),\n",
       " ('tutor', 0.08134999871253967),\n",
       " ('supervisor', 0.08304999768733978),\n",
       " ('teaching', 0.019179999828338623)]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alternates(\"teacher\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('jailed', 0.013559999875724316),\n",
       " ('seized', 0.06790000200271606),\n",
       " ('detained', 0.020649999380111694),\n",
       " ('arrests', 0.023259999230504036),\n",
       " ('suspects', 0.025769999250769615),\n",
       " ('authorities', 0.028310000896453857),\n",
       " ('sentenced', 0.034779999405145645),\n",
       " ('convicted', 0.03928999975323677),\n",
       " ('jail', 0.04715999960899353),\n",
       " ('charged', 0.05299000069499016)]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alternates(\"arrested\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('punched', 0.6401690244674683),\n",
       " ('kicking', 0.5993002653121948),\n",
       " ('slapping', 0.5451561212539673),\n",
       " ('punch', 0.5448700189590454),\n",
       " ('shoving', 0.5408695936203003),\n",
       " ('punches', 0.5125662088394165),\n",
       " ('fists', 0.4962232708930969),\n",
       " ('knocking', 0.4951150417327881),\n",
       " ('yelling', 0.4795556366443634),\n",
       " ('cursing', 0.4713093340396881)]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similar_by_word(\"punching\", topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.09881000220775604"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_bias_score_by_word(\"punching\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/bhavya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_stopwords = stopwords.words('english') \n",
    "gender_pronouns = ['he','him','his','himself','she',\"she's\",'her','hers','herself',]\n",
    "neutral_stopwords = [x for x in nltk_stopwords if x not in gender_pronouns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"last\" in neutral_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.09300000220537186"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_bias_score_by_word(\"uniform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:semantic]",
   "language": "python",
   "name": "conda-env-semantic-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
